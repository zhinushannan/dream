---
标题: 基于CentOS7镜像和数据挂载卷实现Docker搭建Hadoop集群
日期: 2022-06-26
标签: 大数据, Hadoop, 集群
分类: 大数据
关键词: 大数据, Hadoop, 集群
封面：https://pic-go-zhinushannan.oss-cn-hangzhou.aliyuncs.com/202206/202206261620161.png


---



# 实现原理

以一主二从的Hadoop集群为例，在搭建时，需要如下条件：
1. 启动三台机器
2. 三台机器要相互和自身免密登录
3. 每台机器都要有Java和Hadoop的环境，并且Hadoop的配置文件也要相同

基于这个条件，构建一个已经安装好相关软件的CentOS7镜像。
在启动镜像时，设置数据挂载卷到指定目录，作为Java、Hadoop以及以后的Storm、Hive等大数据框架放置的位置。这样，可以实现通过修改宿主机上的配置文件，同步修改三台机器上的配置文件和软件。
同时，通过rsync工具可以实现对`.bashrc`等环境配置的文件进行跨机器拷贝。

# 构建并启动预装必要软件的、可登录的CentOS7容器

## 构建

```dockerfile
# 基于CentOS7镜像
FROM centos:7

MAINTAINER zhinushannan<zhinushannan@gmail.com>

# 设置root用户的登录密码
ENV ROOT_PASSWORD 12345678
ENV TIMEZONE Asia/Shanghai

# 安装rsync、vim、openssh-server、openssh-clients、net-tools，并配置ssh登录密码，创建工作目录
RUN yum install -y rsync vim openssh-server openssh-clients net-tools && \
    echo $ROOT_PASSWORD | passwd --stdin root && \
    ssh-keygen -t dsa -f /etc/ssh/ssh_host_dsa_key && \
    ssh-keygen -t rsa -f /etc/ssh/ssh_host_rsa_key && \
    ln -snf /usr/share/zoneinfo/$TIMEZONE /etc/localtime && echo $TIMEZONE > /etc/timezone && \
    mkdir -p /opt/hadoop_dir/name /opt/hadoop_dir/data /opt/hadoop_dir/logs /opt/hadoop_dir/pids /opt/hadoop_dir/temp  

# 容器启动时，启动sshd服务
CMD ["/usr/sbin/sshd", "-D"]

# 监听22端口
EXPOSE 22

```

## 启动

新建一个网络
```shell
docker network create hadoop_network
```

通过如下命令启动三台相同数据挂载卷的容器
```shell
docker run -d --name [container_name] --network [network_name] --volume [localVolume]:[containerVolume] -e TZ=Asia/Shanghai image_name
```
例如：
```shell
docker run -d --name master --network hadoop_test --volume /Users/zhinushannan/code/hadoop/module:/opt/module -e TZ=Asia/Shanghai centos_ssh
docker run -d --name slave1 --network hadoop_test --volume /Users/zhinushannan/code/hadoop/module:/opt/module -e TZ=Asia/Shanghai centos_ssh
docker run -d --name slave2 --network hadoop_test --volume /Users/zhinushannan/code/hadoop/module:/opt/module -e TZ=Asia/Shanghai centos_ssh
```

# 配置免密登录和环境

## 配置免密登录

在每一台机器上生成公钥私钥`ssh-keygen -t rsa`，执行这个命令之后，需要连续按三次回车。

在每一台机器人上执行如下命令，将公钥拷贝到需要免密登录的机器上，从而实现免密登录。需要按照提示输入对方用户的登录密码。
```shell
ssh-copy-id master
ssh-copy-id slave1
ssh-copy-id slave2
```

## 拷贝相关软件
将适合自己架构的Java和Hadoop解压并拷贝进相关目录。

![2数据挂载卷](https://pic-go-zhinushannan.oss-cn-hangzhou.aliyuncs.com/202206/202206261621308.png)

## 配置环境变量

通过ssh登录其中一台机器，修改`/etc/profile`文件，在最后添加：
```shell
export JAVA_HOME=/opt/module/jdk1.8.0_333/
export PATH=$PATH:$JAVA_HOME/bin/

export HADOOP_HOME=/opt/module/hadoop-3.3.1/
export PATH=$PATH:$HADOOP_HOME/bin/:$HADOOP_HOME/sbin/
```
添加完成后编译一下该文件，执行`java -version`和`hadoop version`指令检测是否配置成功。

利用`rsync`工具将环境配置文件拷贝到其他机器上：
```shell
rsync -v /etc/profile slave1:/etc/profile
rsync -v /etc/profile slave2:/etc/profile
```
进入其他机器编译环境配置文件，并检测环境是否生效。


# 配置Hadoop

## 集群部署规划

注意：
- NameNode和SecondaryNameNode不要部署在同一台机器上
- ResourceManager很消耗内存，不要和NameNode、SecondaryNameNode部署在同一台机器上

|  | master | slave1 | slave2 |
| :--: | :--: | :--: | :--: |
| HDFS | NameNode、DataNode | DataNode | SecondaryNameNode、DataNode |
| YARN | NodeManager | ResourceManager、NodeManager | NodeManager |

## 配置文件说明

Hadoop 配置文件分两类：默认配置文件和自定义配置文件，只有用户想修改某一默认配置值时，才需要修改自定义配置文件，更改相应属性值。

### 默认配置文件

| 要获取的默认文件 | 文件存放在Hadoop的jar包中的位置 |
| :--: | :--: |
| core-default.xml | hadoop-common-3.3.1.jar/core-default.xml |
| hdfs-default.xml | hadoop-hdfs-3.3.1.jar/hdfs-default.xml |
| yarn-default.xml | hadoop-yarn-3.3.1.jar/yarn-default.xml |
| mapred-default.xml | hadoop-mapred-3.3.1.jar/mapred-default.xml |


### 自定义配置文件

`core-site.xml`、`hdfs-site.xml`、`yarn-site.xml`、`mapred-site.xml`四个配置文件存放在`$HADOOP_HOME/etc/hadoop`这个路径上，用户可以根据项目需求重新进行修改配置。

## 修改配置文件

### 配置`$HADOOP_HOME/etc/hadoop/hadoop-env.sh`

设置hadoop运行所需环境变量

修改其中的：

```shell
export JAVA_HOME=/opt/module/jdk1.8.0_333/  # jdk存储目录，根据自己的机器来填写
export HADOOP_LOG_DIR=/opt/hadoop_dir/logs  # 日志存储目录，根据dockerfile中的环境变量填写
export HADOOP_PID_DIR=/opt/hadoop_dir/pids  # pid文件存储目录，根据dockerfile中的环境变量填写

# 如下内容追加在文件末尾（运行用户设置）
# user
export HDFS_NAMENODE_USER=root
export HDFS_DATANODE_USER=root
export HDFS_SECONDARYNAMENODE_USER=root
export YARN_RESOURCEMANEGER_USER=root
export YARN_NODEMANEGER_USER=root
```

### 配置`$HADOOP_HOME/etc/hadoop/workers`

设置数据节点服务器（datanode）的主机信息

```shell
slave1
slave2
```

### 配置``$HADOOP_HOME/etc/hadoop/core-site.xml`

配置hadoop集群的全局参数

```xml
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<configuration>
    <!-- 指定NameNode的地址 -->
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://master:9000</value>
    </property>

    <!-- 指定hadoop数据的存储目录，根据dockerfile填写 -->
    <property>
        <name>hadoop.tmp.dir</name>
        <value>/opt/hadoop_dir/temp</value>
    </property>
  
    <!-- 用户代理机制：表示root用户可以代理主机上的所有用户 -->
    <property>
        <name>hadoop.proxyuser.root.hosts</name>
        <value>*</value>
    </property>
  
    <!-- 用户代理机制：表示root组可以代理主机上的所有组 -->
    <property>
        <name>hadoop.proxyuser.root.groups</name>
        <value>*</value>
    </property>

</configuration>
```

### 配置`$HADOOP_HOME/etc/hadoop/hdfs-site.xml`

设置HDFS（hadoop分布式文件系统）参数

```xml
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<configuration>
    
    <!-- NameNode 原数据存储位置 -->
    <property>
        <name>dfs.namenode.name.dir</name>
        <value>/opt/hadoop_dir/name</value>
    </property>
  
  	<!-- DataNode在本地磁盘存放block的位置 -->
    <property>
        <name>dfs.datanode.data.dir</name>
        <value>/opt/hadoop_dir/data</value>
    </property>
  
    <!-- 备份数：即在文件被写入的时候，每一块将要被复制多少份  -->
    <property>
        <name>dfs.replication</name>
        <value>2</value>
    </property>

  	<!-- secondary namenode HTTP服务器地址和端口 -->
    <property>
        <name>dfs.namenode.secondary.http-address</name>
        <value>master:9001</value>
    </property>
  
  	<!-- 是否允许在namenode和datanode中启用WebHDFS (REST API) -->
    <property>
        <name>dfs.webhdfs.enabled</name>
        <value>true</value>
    </property>  
  
    <!-- 当为true时，则允许HDFS的检测，当为false时，则关闭HDFS的检测，但不影响其它HDFS的其它功能。 -->
    <property>
        <name>dfs.permissions</name>
        <value>false</value>
    </property>  

    <!-- namenode HTTP服务器地址和端口 -->
    <property>
        <name>dfs.http.address</name>
        <value>0.0.0.0:50070</value>
    </property>  

</configuration>
```

### 配置`$HADOOP_HOME/etc/hadoop/yarn-site.xml`

```xml
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<configuration>
    <!-- 指定MR走shuffle -->
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>

    <!-- 指定ResourceManager的地址-->
    <property>
        <name>yarn.resourcemanager.hostname</name>
        <value>master</value>
    </property>

</configuration>
```

### 配置`$HADOOP_HOME/etc/hadoop/mapred-site.xml`

```xml
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<configuration>
    <!-- 指定MapReduce程序运行在Yarn上 -->
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
  
    <!-- yarn运行时参数 -->
    <property>
        <name>yarn.app.mapreduce.am.env</name>
        <value>HADOOP_MAPRED_HOME=${HADOOP_HOME}</value>
    </property>  
  
    <!-- map 运行时参数 -->
    <property>
        <name>mapreduce.map.env</name>
        <value>HADOOP_MAPRED_HOME=${HADOOP_HOME}</value>
    </property>
  
    <!-- reduce 运行时参数 -->
    <property>
        <name>mapreduce.reduce.env</name>
        <value>HADOOP_MAPRED_HOME=${HADOOP_HOME}</value>
    </property>  
  
</configuration>
```


# 启动集群

## 启动集群

在机器master上：
初始化`namenode`：

```shell
hdfs namenode -format
```
启动集群：
```shell
start-all.sh
```

停止集群：

```shell
stop-all.sh
```



## WEB端界面查看

在WEB端查看HDFS的NameNode：`http://[hadoop01_ip]:9870`
在WEB端查看YARN的ResourceManager：`http://[hadoop02_ip]:8088`

## 错误解决

### 提示没有JAVA_HOME
在`$HADOOP_HOME/etc/hadoop/hadoop-env.sh`中的`export JAVA_HOME=`后手动填写`JAVA_HOME`的值。

### 启动HDFS报错
```shell
[root@8609fa1a900d hadoop-3.3.1]# sbin/start-dfs.sh
Starting namenodes on [hadoop01]
ERROR: Attempting to operate on hdfs namenode as root
ERROR: but there is no HDFS_NAMENODE_USER defined. Aborting operation.
Starting datanodes
ERROR: Attempting to operate on hdfs datanode as root
ERROR: but there is no HDFS_DATANODE_USER defined. Aborting operation.
Starting secondary namenodes [hadoop03]
ERROR: Attempting to operate on hdfs secondarynamenode as root
ERROR: but there is no HDFS_SECONDARYNAMENODE_USER defined. Aborting operation.
2022-06-26 08:08:02,653 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
```

在 `$HADOOP_HOME/sbin` 路径下，将 `start-dfs.sh` 、 `stop-dfs.sh` 两个文件顶部添加以下参数
```shell
HDFS_DATANODE_USER=root
HADOOP_SECURE_DN_USER=hdfs
HDFS_NAMENODE_USER=root
HDFS_SECONDARYNAMENODE_USER=root
```

### 启动YARN报错

```shell
[root@0fd5f4a17b7d hadoop-3.3.1]# start-yarn.sh
Starting resourcemanager
ERROR: Attempting to operate on yarn resourcemanager as root
ERROR: but there is no YARN_RESOURCEMANAGER_USER defined. Aborting operation.
Starting nodemanagers
ERROR: Attempting to operate on yarn nodemanager as root
ERROR: but there is no YARN_NODEMANAGER_USER defined. Aborting operation.
```

在`start-yarn.sh`，`stop-yarn.sh`顶部添加以下参数：
```shell
YARN_RESOURCEMANAGER_USER=root
HADOOP_SECURE_DN_USER=yarn
YARN_NODEMANAGER_USER=root
```

# 完全删除集群

```shell
# 停止容器
docker stop hadoop01 hadoop02 hadoop03
# 删除容器
docker rm hadoop01 hadoop02 hadoop03
# 删除网络
docker network rm hadoop_test
# 删除数据卷（或者保留供下次使用）
rm -r /Users/zhinushannan/code/hadoop/module
```



# 使用Docker-Compose群起固定IPHadoop集群

```yaml

version: "2.6"  # docker-compose 版本号

services:
  master:  # 服务名
    image: centos_hadoop  # 镜像
    container_name: master  # 容器名
    hostname: master
    volumes:
      - /Users/zhinushannan/code/hadoop/module:/opt/module
    networks:
      test:
        ipv4_address: 172.18.0.2

  slave1:  # 服务名
    image: centos_hadoop  # 镜像
    container_name: slave1  # 容器名
    hostname: slave1
    volumes:
      - /Users/zhinushannan/code/hadoop/module:/opt/module
    networks:
      test:
        ipv4_address: 172.18.0.3

  slave2:  # 服务名
    image: centos_hadoop  # 镜像
    container_name: slave2  # 容器名
    hostname: slave2
    volumes:
      - /Users/zhinushannan/code/hadoop/module:/opt/module
    networks:
      test:
        ipv4_address: 172.18.0.4

networks:
   test:
      ipam:
         config:
           - subnet: 172.18.0.0/16


```

