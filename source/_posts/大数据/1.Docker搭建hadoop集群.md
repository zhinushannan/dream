---
title: 基于CentOS7镜像和数据挂载卷实现Docker搭建Hadoop集群
urlname: 20220625
date: 2022-06-25 08:29:07
tags: [大数据, Hadoop, 集群]
categories: [大数据]
keywords: [大数据, Hadoop, 集群]
top_img: /images/大数据/1docker搭建hadoop集群/cover.png
cover: /images/大数据/1docker搭建hadoop集群/cover.png
---

# 实现原理

以一主二从的Hadoop集群为例，在搭建时，需要如下条件：
1. 启动三台机器
2. 三台机器要相互和自身免密登录
3. 每台机器都要有Java和Hadoop的环境，并且Hadoop的配置文件也要相同

基于这个条件，构建一个已经安装好相关软件的CentOS7镜像。
在启动镜像时，设置数据挂载卷到指定目录，作为Java、Hadoop以及以后的Storm、Hive等大数据框架放置的位置。这样，可以实现通过修改宿主机上的配置文件，同步修改三台机器上的配置文件和软件。
同时，通过rsync工具可以实现对`.bashrc`等环境配置的文件进行跨机器拷贝。

# 构建并启动预装必要软件的、可登录的CentOS7容器

## 构建

```dockerfile
# 基于CentOS7镜像
FROM centos:7

MAINTAINER zhinushannan<zhinushannan@gmail.com>

# 设置root用户的登录密码
ENV ROOT_PASSWORD 12345678

# 安装rsync、vim、openssh-server、openssh-clients、net-tools，并配置ssh登录密码
RUN yum install -y rsync vim openssh-server openssh-clients net-tools && \
    echo $ROOT_PASSWORD | passwd --stdin root && \
    ssh-keygen -t dsa -f /etc/ssh/ssh_host_dsa_key && \
    ssh-keygen -t rsa -f /etc/ssh/ssh_host_rsa_key

# 容器启动时，启动sshd服务
CMD ["/usr/sbin/sshd", "-D"]

# 监听22端口
EXPOSE 22

```

## 启动

新建一个网络
```shell
docker network create hadoop_network
```

通过如下命令启动三台相同数据挂载卷的容器
```shell
docker run -d --name [container_name] --network [network_name] --volume [localVolume]:[containerVolume] image_name
```
例如：
```shell
docker run -d --name hadoop01 --network hadoop_test --volume /Users/zhinushannan/code/hadoop/module:/opt/module centos_ssh
docker run -d --name hadoop02 --network hadoop_test --volume /Users/zhinushannan/code/hadoop/module:/opt/module centos_ssh
docker run -d --name hadoop03 --network hadoop_test --volume /Users/zhinushannan/code/hadoop/module:/opt/module centos_ssh
```

# 配置免密登录和环境

## 配置免密登录

在每一台机器上生成公钥私钥`ssh-keygen -t rsa`，执行这个命令之后，需要连续按三次回车。

在每一台机器人上执行如下命令，将公钥拷贝到需要免密登录的机器上，从而实现免密登录。需要按照提示输入对方用户的登录密码。
```shell
ssh-copy-id hadoop01
ssh-copy-id hadoop02
ssh-copy-id hadoop03
```

## 拷贝相关软件
将适合自己架构的Java和Hadoop解压并拷贝进相关目录。

![2数据挂载卷](/images/大数据/1docker搭建hadoop集群/2数据挂载卷.png)

## 配置环境变量

通过ssh登录其中一台机器，修改`~/.bashrc`文件，在最后添加：
```shell
export JAVA_HOME=/opt/module/jdk1.8.0_333/
export PATH=$PATH:$JAVA_HOME/bin/

export HADOOP_HOME=/opt/module/hadoop-3.3.3/
export PATH=$PATH:$HADOOP_HOME/bin/:$HADOOP_HOME/sbin/
```
添加完成后编译一下该文件，执行`java -version`和`hadoop version`指令检测是否配置成功。

利用`rsync`工具将环境配置文件拷贝到其他机器上：
```shell
rsync -v ~/.bashrc hadoop02:/root/.bashrc
rsync -v ~/.bashrc hadoop03:/root/.bashrc
```
进入其他机器编译环境配置文件，并检测环境是否生效。


# 配置Hadoop

## 集群部署规划

注意：
- NameNode和SecondaryNameNode不要部署在同一台机器上
- ResourceManager很消耗内存，不要和NameNode、SecondaryNameNode部署在同一台机器上

|  | hadoop01 | hadoop02 | hadoop03 |
| :--: | :--: | :--: | :--: |
| HDFS | NameNode、DataNode | DataNode | SecondaryNameNode、DataNode |
| YARN | NodeManager | ResourceManager、NodeManager | NodeManager |

## 配置文件说明

Hadoop 配置文件分两类：默认配置文件和自定义配置文件，只有用户想修改某一默认配置值时，才需要修改自定义配置文件，更改相应属性值。

### 默认配置文件

| 要获取的默认文件 | 文件存放在Hadoop的jar包中的位置 |
| :--: | :--: |
| core-default.xml | hadoop-common-3.3.3.jar/core-default.xml |
| hdfs-default.xml | hadoop-hdfs-3.3.3.jar/hdfs-default.xml |
| yarn-default.xml | hadoop-yarn-3.3.3.jar/yarn-default.xml |
| mapred-default.xml | hadoop-mapred-3.3.3.jar/mapred-default.xml |


### 自定义配置文件

`core-site.xml`、`hdfs-site.xml`、`yarn-site.xml`、`mapred-site.xml`四个配置文件存放在`$HADOOP_HOME/etc/hadoop`这个路径上，用户可以根据项目需求重新进行修改配置。

### 修改配置文件

配置`core-site.xml`
```xml
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<configuration>
    <!-- 指定NameNode的地址 -->
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://hadoop01:8020</value>
    </property>

    <!-- 指定hadoop数据的存储目录 -->
    <property>
        <name>hadoop.tmp.dir</name>
        <value>/opt/module/hadoop-3.3.3/data</value>
    </property>

    <!-- 配置HDFS网页登录使用的静态用户为atguigu -->
    <property>
        <name>hadoop.http.staticuser.user</name>
        <value>root</value>
    </property>
</configuration>
```

配置`hdfs-site.xml`
```xml
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<configuration>
    <!-- nn web端访问地址-->
    <property>
        <name>dfs.namenode.http-address</name>
        <value>hadoop01:9870</value>
    </property>
    <!-- 2nn web端访问地址-->
    <property>
        <name>dfs.namenode.secondary.http-address</name>
        <value>hadoop03:9868</value>
    </property>
</configuration>
```

配置`yarn-site.xml`
```xml
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<configuration>
    <!-- 指定MR走shuffle -->
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>

    <!-- 指定ResourceManager的地址-->
    <property>
        <name>yarn.resourcemanager.hostname</name>
        <value>hadoop02</value>
    </property>

    <!-- 环境变量的继承 -->
    <property>
        <name>yarn.nodemanager.env-whitelist</name>
        <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME</value>
    </property>
</configuration>
```

配置`mapred-site.xml`
```xml
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<configuration>
    <!-- 指定MapReduce程序运行在Yarn上 -->
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
</configuration>
```

配置`workers`
```text
hadoop01
hadoop02
hadoop03
```

# 启动集群

## 启动集群

在机器hadoop01上：
初始化`namenode`：
```shell
hdfs namenode -format
```
启动`hdfs`：
```shell
sbin/start-dfs.sh
```

在机器hadoop02上：
启动YARN：
```shell
sbin/start-yarn.sh
```

## WEB端界面查看

在WEB端查看HDFS的NameNode：`http://[hadoop01_ip]:9870`
在WEB端查看YARN的ResourceManager：`http://[hadoop02_ip]:8088`

## 错误解决

### 提示没有JAVA_HOME
在`$HADOOP_HOME/etc/hadoop/hadoop-env.sh`中的`export JAVA_HOME=`后手动填写`JAVA_HOME`的值。

### 启动HDFS报错
```shell
[root@8609fa1a900d hadoop-3.3.3]# sbin/start-dfs.sh
Starting namenodes on [hadoop01]
ERROR: Attempting to operate on hdfs namenode as root
ERROR: but there is no HDFS_NAMENODE_USER defined. Aborting operation.
Starting datanodes
ERROR: Attempting to operate on hdfs datanode as root
ERROR: but there is no HDFS_DATANODE_USER defined. Aborting operation.
Starting secondary namenodes [hadoop03]
ERROR: Attempting to operate on hdfs secondarynamenode as root
ERROR: but there is no HDFS_SECONDARYNAMENODE_USER defined. Aborting operation.
2022-06-26 08:08:02,653 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
```

在 `$HADOOP_HOME/sbin` 路径下，将 `start-dfs.sh` 、 `stop-dfs.sh` 两个文件顶部添加以下参数
```shell
HDFS_DATANODE_USER=root
HADOOP_SECURE_DN_USER=hdfs
HDFS_NAMENODE_USER=root
HDFS_SECONDARYNAMENODE_USER=root
```

### 启动YARN报错

```shell
[root@0fd5f4a17b7d hadoop-3.3.3]# start-yarn.sh
Starting resourcemanager
ERROR: Attempting to operate on yarn resourcemanager as root
ERROR: but there is no YARN_RESOURCEMANAGER_USER defined. Aborting operation.
Starting nodemanagers
ERROR: Attempting to operate on yarn nodemanager as root
ERROR: but there is no YARN_NODEMANAGER_USER defined. Aborting operation.
```

在`start-yarn.sh`，`stop-yarn.sh`顶部添加以下参数：
```shell
YARN_RESOURCEMANAGER_USER=root
HADOOP_SECURE_DN_USER=yarn
YARN_NODEMANAGER_USER=root
```

# 完全删除集群

```shell
# 停止容器
docker stop hadoop01 hadoop02 hadoop03
# 删除容器
docker rm hadoop01 hadoop02 hadoop03
# 删除网络
docker network rm hadoop_test
# 删除数据卷（或者保留供下次使用）
rm -r /Users/zhinushannan/code/hadoop/module
```

# 使用Docker-Compose群起固定IPHadoop集群

```yaml

version: "2.6"  

services:
  hadoop01:  
    image: centos_hadoop  
    container_name: hadoop01  
    volumes:
      - /Users/zhinushannan/code/hadoop/module:/opt/module
    networks:
      test:
        ipv4_address: 172.18.0.2

  hadoop02:  
    image: centos_hadoop  
    container_name: hadoop02  
    volumes:
      - /Users/zhinushannan/code/hadoop/module:/opt/module
    networks:
      test:
        ipv4_address: 172.18.0.3

  hadoop03:  
    image: centos_hadoop  
    container_name: hadoop03  
    volumes:
      - /Users/zhinushannan/code/hadoop/module:/opt/module
    networks:
      test:
        ipv4_address: 172.18.0.4

networks:
   test:
      ipam:
         config:
           - subnet: 172.18.0.0/16


```

